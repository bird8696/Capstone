import openai
import os
import time
import wave
import pyaudio
import speech_recognition as sr
from dotenv import load_dotenv

# âœ… `.env` íŒŒì¼ ë¡œë“œ
load_dotenv()

# âœ… í™˜ê²½ ë³€ìˆ˜ì—ì„œ API í‚¤ ê°€ì ¸ì˜¤ê¸°
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")

if not OPENAI_API_KEY:
    raise ValueError("âŒ API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤! `.env` íŒŒì¼ì„ í™•ì¸í•˜ì„¸ìš”.")

# âœ… OpenAI API í´ë¼ì´ì–¸íŠ¸ ìƒì„±
client = openai.OpenAI(api_key=OPENAI_API_KEY)

# âœ… ìŒì„± ì €ì¥ í´ë” ì„¤ì •
RECORDING_DIR = "E:/Capstone2/Capstone/ai_model/recordings"
os.makedirs(RECORDING_DIR, exist_ok=True)

# âœ… ë©´ì ‘ ì§ˆë¬¸ ìƒì„± í•¨ìˆ˜
def generate_interview_question():
    prompt = "FastAPI, SQL, Python, ì„œë²„ êµ¬ì¡° ê´€ë ¨ ê¸°ìˆ  ë©´ì ‘ ì§ˆë¬¸ í•˜ë‚˜ë§Œ ìƒì„±í•´ì£¼ì„¸ìš”."

    response = client.chat.completions.create(
        model="gpt-4",
        messages=[
            {"role": "system", "content": "ë‹¹ì‹ ì€ ë©´ì ‘ê´€ì…ë‹ˆë‹¤."},
            {"role": "user", "content": prompt}
        ]
    )

    return response.choices[0].message.content

# âœ… ìŒì„± ë…¹ìŒ ë° STT ë³€í™˜ í•¨ìˆ˜
def record_audio(question_num):
    """
    1. ìµœëŒ€ 60ì´ˆ ë™ì•ˆ ë…¹ìŒ
    2. 10ì´ˆ ì´ìƒ ë§ì´ ì—†ìœ¼ë©´ ìë™ ì¢…ë£Œ
    3. ë…¹ìŒëœ ìŒì„±ì„ íŒŒì¼ë¡œ ì €ì¥
    """
    CHUNK = 1024
    FORMAT = pyaudio.paInt16
    CHANNELS = 1
    RATE = 44100
    RECORD_SECONDS = 60  # ìµœëŒ€ ë…¹ìŒ ì‹œê°„
    SILENCE_LIMIT = 10  # 10ì´ˆ ì´ìƒ ë¬´ìŒì´ë©´ ì¢…ë£Œ

    p = pyaudio.PyAudio()
    stream = p.open(format=FORMAT, channels=CHANNELS,
                    rate=RATE, input=True,
                    frames_per_buffer=CHUNK)

    frames = []
    start_time = time.time()
    silence_start = None

    print(f"ğŸ™ï¸ ë‹µë³€ì„ ë§í•´ì£¼ì„¸ìš” (ìµœëŒ€ {RECORD_SECONDS}ì´ˆ)...")

    while True:
        data = stream.read(CHUNK)
        frames.append(data)

        elapsed_time = time.time() - start_time

        # ğŸ” **ë¬´ìŒ ê°ì§€ (10ì´ˆ ë™ì•ˆ ì•„ë¬´ ì†Œë¦¬ë„ ì—†ìœ¼ë©´ ì¢…ë£Œ)**
        audio_energy = sum(abs(int.from_bytes(data[i:i+2], 'little', signed=True))
                           for i in range(0, len(data), 2)) / CHUNK
        if audio_energy < 500:  # ê¸°ë³¸ ì„ê³„ê°’ë³´ë‹¤ ì‘ì€ ê²½ìš° ë¬´ìŒìœ¼ë¡œ íŒë‹¨
            if silence_start is None:
                silence_start = time.time()
            elif time.time() - silence_start >= SILENCE_LIMIT:
                print("â­ï¸ 10ì´ˆ ë™ì•ˆ ì…ë ¥ì´ ì—†ì–´ ë‹¤ìŒ ì§ˆë¬¸ìœ¼ë¡œ ë„˜ì–´ê°‘ë‹ˆë‹¤.")
                break
        else:
            silence_start = None  # ë‹¤ì‹œ ë§í•˜ë©´ ë¬´ìŒ íƒ€ì´ë¨¸ ë¦¬ì…‹

        if elapsed_time >= RECORD_SECONDS:
            print("â¹ï¸ ìµœëŒ€ ì‘ë‹µ ì‹œê°„(60ì´ˆ) ë„ë‹¬. ë…¹ìŒì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
            break

    stream.stop_stream()
    stream.close()
    p.terminate()

    # ğŸ¤ **ë…¹ìŒëœ ì˜¤ë””ì˜¤ ì €ì¥**
    audio_path = os.path.join(RECORDING_DIR, f"answer_{question_num}.wav")
    wf = wave.open(audio_path, 'wb')
    wf.setnchannels(CHANNELS)
    wf.setsampwidth(p.get_sample_size(FORMAT))
    wf.setframerate(RATE)
    wf.writeframes(b''.join(frames))
    wf.close()

    print(f"ğŸ’¾ ë…¹ìŒ íŒŒì¼ ì €ì¥ë¨: {audio_path}")
    return audio_path

# âœ… ìŒì„± ì¸ì‹ (STT) ë³€í™˜ í•¨ìˆ˜
def recognize_speech(audio_path):
    recognizer = sr.Recognizer()

    with sr.AudioFile(audio_path) as source:
        audio = recognizer.record(source)  # ì „ì²´ ì˜¤ë””ì˜¤ ì½ê¸°

    try:
        response_text = recognizer.recognize_google(audio, language="ko-KR")
        return response_text
    except sr.UnknownValueError:
        return "ìŒì„±ì„ ì¸ì‹í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."
    except sr.RequestError:
        return "STT ì„œë¹„ìŠ¤ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."

# âœ… ë©´ì ‘ ì‹¤í–‰ í•¨ìˆ˜
def start_interview():
    print("\nğŸ› ï¸ [ë°±ì—”ë“œ ê°œë°œì ë©´ì ‘ ì‹œì‘]\n")

    for i in range(3):  # 3ê°œ ì§ˆë¬¸ ì˜ˆì œ
        question = generate_interview_question()
        print(f"\nğŸ”¹ ì§ˆë¬¸ {i+1}: {question}")

        audio_file = record_audio(i + 1)
        answer = recognize_speech(audio_file)

        print(f"ğŸ—£ï¸ ì‘ë‹µ: {answer}")

# âœ… ì‹¤í–‰
if __name__ == "__main__":
    start_interview()
